{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>  \n",
    "<img src=\"title.jpg\", width =1000>\n",
    "<img src=\"5dc1833201bb8.jpg\", width =1000>\n",
    "<img src=\"name2.jpg\", width = 1000>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction\n",
    "- ANN\n",
    "  *  \n",
    "  * Types of ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"deep-learning.jpg\", width = 1000/>\n",
    "</div>\n",
    "\n",
    "## What Is Deep Learning?\n",
    "by definition deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making. Deep learning is a subset of machine learning in AI that deals with algorithms inspired by the structure and function of the brain called artificial neural networks which are capable of learning unsupervised from data that is unstructured/unlabeled.\n",
    "\n",
    "Deep learning algorithms are similar to the structure of the nervous system where neurons are connected to another and pass information to each other in the sense that deep learning algorithms work in layers and a typical model atleast have three layers. Each layer accepts the information from previous and pass it on to the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Machine Learning and Deep Learning\n",
    "\n",
    "### 1. Data Dependencies\n",
    "Performance is the main key difference between both learning approaches.\n",
    "`Deep learning` algorithms perform a lot better compared to conventional machine learning algorithms as more data becomes available.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"Deep-Learning-VS-Conventional-Algorithms.ppm\", width = 600/>\n",
    "</div>\n",
    "\n",
    "### 2. Hardware Dependencies\n",
    "Generally, Deep Learning depends on high-end machines while traditional learning depends on low-end machines. Thus, Deep Learning requirement includes GPUs. That is an integral part of it’s working. They also do a large amount of matrix multiplication operations.\n",
    "\n",
    "### 3. Problem Solving Approach\n",
    "Generally, we use the traditional algorithm to solve problems. However, it needs to break a problem into different parts to solve them individually. To get a result, combine them all.\n",
    "\n",
    "For Example:\n",
    "\n",
    "Let us suppose you have a task of multiple object detection. In this task, we have to identify what the object is and where is it present in the image. In a Machine Learning approach, we have to divide the problem into two steps:\n",
    "\n",
    "- object detection\n",
    "- object recognition\n",
    "\n",
    "First, we use the grabcut algorithm to skim through the image and find all the possible objects. Then, of all the recognized objects, you would use an object recognition algorithm like SVM with HOG to recognize relevant objects.\n",
    "\n",
    "### 4. Execution Time\n",
    "Usually, Deep Learning takes more time to train as compared to Machine Learning. The main reason is that there are so many parameters in a Deep Learning algorithm. Whereas Machine Learning takes much less time to train, ranging from a few seconds to a few hours.\n",
    "\n",
    "### 5. Interpretability\n",
    "We have interpretability as a factor for the comparison of both learning techniques. However, Deep Learning is still thought 10 times before its use in industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked into what deep learning is and compared it with machine learning, we can now turn our attention Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Artificial Neural Network (ANN)?\n",
    "\n",
    "Deep Learning is used in layers to create an Artificial “Neural Network” that can learn and make intelligent decisions on its own.\n",
    "<div>\n",
    "<img src=\"1_Io_kQ3nkdwnHNG6RgJlIDQ.png\", width = 600/>\n",
    "</div>\n",
    "\n",
    "An artificial neural network (ANN) is the piece of a computing system designed to simulate the way the human brain analyzes and processes information. It is the foundation of artificial intelligence (AI) and solves problems that would prove impossible or difficult by human or statistical standards. ANNs have self-learning capabilities that enable them to produce better results as more data becomes available.\n",
    "\n",
    "\n",
    "\n",
    "**Defining a Neural Network**\n",
    "\n",
    "A neural network is structured like the human brain and consists of artificial neurons, also known as nodes. These nodes are stacked next to each other in three layers:\n",
    "\n",
    "- The input layer \n",
    "- The hidden layer(s)\n",
    "- The output layer\n",
    "<div>\n",
    "<img src=\"defining ANN.png\", width = 600/>\n",
    "</div>\n",
    "\n",
    "Data provides each node with information in the form of inputs. The node multiplies the inputs with random weights, calculates them, and adds a bias. Finally, nonlinear functions, also known as activation functions, are applied to determine which neuron to fire\n",
    "\n",
    "\n",
    "**A biological neuron in comparison to an artificial neural network**\n",
    "<div>\n",
    "<img src=\"A-biological-neuron-in-comparison-to-an-artificial-neural-network-a-human-neuron-b.png\", width = 600/>\n",
    "</div>\n",
    "\n",
    "                                    (a) human neuron                (b) artificial neuron\n",
    "\n",
    "                                    (c) biological synapse          (d) ANN synapses.\n",
    "                                    \n",
    "Biological neurons have dendrites to receive signals, a cell body to process them,\n",
    "and an axon/axon terminal to transfer signals out to other neurons. Similarly an artificial\n",
    "neuron has multiple input channels to accept training samples represented as a vector,\n",
    "and a processing stage where the weights(w) are adjusted such that the output error\n",
    "(actual vs. predicted) is minimized. Then the result is fed into an activation function\n",
    "to produce output, for example, a classification label. The activation function for a\n",
    "classification problem is a threshold cutoff (standard is .5) above which class is 1 else 0\n",
    "\n",
    "**Why are neural networks important?**\n",
    "\n",
    "Neural Networks can recognize hidden patterns and correlations in raw data, cluster and classify it, and – over time – continuously learn and improve.\n",
    "\n",
    "\n",
    "Neural networks are also ideally suited to help people solve complex problems in real-life situations. They can learn and model the relationships between inputs and outputs that are nonlinear and complex; make generalizations and inferences; reveal hidden relationships, patterns and predictions; and model highly volatile data (such as financial time series data) and variances needed to predict rare events (such as fraud detection). As a result, neural networks can improve decision processes in areas such as\n",
    "\n",
    "- Credit card and Medicare fraud detection.\n",
    "- Optimization of logistics for transportation networks.\n",
    "- Character and voice recognition, also known as natural language processing.\n",
    "- Medical and disease diagnosis.\n",
    "- Targeted marketing.\n",
    "- Financial predictions for stock prices, currency, options, futures, bankruptcy and bond ratings.\n",
    "- Robotic control systems\n",
    "- Electrical load and energy demand forecasting.\n",
    "- Process and quality control.\n",
    "- Chemical compound identification.\n",
    "- Ecosystem evaluation.\n",
    "- Computer vision to interpret raw photos and videos (for example, in medical imaging and robotics and facial recognition).\n",
    "\n",
    "**Deep Neural Networks**\n",
    "\n",
    "A deep neural network is a neural network with a certain level of complexity, a neural network with more than two layers. Deep neural networks use sophisticated mathematical modeling to process data in complex ways.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"cover.png\", width = 600/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Deep Learning Algorithms Work?\n",
    "\n",
    "While deep learning algorithms feature self-learning representations, they depend upon ANNs that mirror the way the brain computes information. During the training process, algorithms use unknown elements in the input distribution to extract features, group objects, and discover useful data patterns. Much like training machines for self-learning, this occurs at multiple levels, using the algorithms to build the models.\n",
    "\n",
    "Deep learning models make use of several algorithms. While no one network is considered perfect, some algorithms are better suited to perform specific tasks. To choose the right ones, it’s good to gain a solid understanding of all primary algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Deep Learning Algorithms\n",
    "\n",
    "1. Convolutional Neural Networks (CNNs)\n",
    "2. Long Short Term Memory Networks (LSTMs)\n",
    "3. Recurrent Neural Networks (RNNs)\n",
    "4. Generative Adversarial Networks (GANs)\n",
    "5. Radial Basis Function Networks (RBFNs)\n",
    "6. Multilayer Perceptrons (MLPs)\n",
    "8. Self Organizing Maps (SOMs)\n",
    "9. Deep Belief Networks (DBNs)\n",
    "10. Restricted Boltzmann Machines( RBMs)\n",
    "11. Autoencoders\n",
    "12. Deep learning algorithms work with almost any kind of data and require large amounts of computing power and information to solve complicated issues.\n",
    "\n",
    "Before we deep-dive into the types of deep learning algorithms, Let's start buy buildig  simple Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Sklearn Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1]\n",
      "Actual: [0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1]\n",
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# import sklearn.linear_model.perceptron\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import perceptron\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# Let's use sklearn make_classification function to create some test data.\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(20, 2, 2, 0, weights=[.5, .5], random_state=2021)\n",
    "\n",
    "# Create the model\n",
    "clf = perceptron.Perceptron(verbose=0, random_state=2017, fit_intercept=True, eta0=0.002)\n",
    "clf.fit(X,y)\n",
    "print(\"Prediction: \" + str(clf.predict(X)))\n",
    "print(\"Actual: \" + str(y))\n",
    "print(\"Accuracy: \" + str(clf.score(X, y)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 Coefficient: -0.00025267927743280894\n",
      "X2 Coefficient: 0.006362739685363655\n",
      "Intercept: [-0.002]\n"
     ]
    }
   ],
   "source": [
    "# Output the values\n",
    "print(\"X1 Coefficient: \" + str(clf.coef_[0,0]))\n",
    "print(\"X2 Coefficient: \" + str(clf.coef_[0,1]))\n",
    "print(\"Intercept: \" + str(clf.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drawback of the single perceptron approach is that it can only learn linearly\n",
    "separable functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons (Feedforward Neural Network)\n",
    "\n",
    "To address the drawback of single perceptrons, multilayer perceptrons were proposed;\n",
    "also commonly known as a feedforward neural network, it is a composition of multiple\n",
    "perceptrons connected in different ways and operating on distinctive activation functions\n",
    "to enable improved learning mechanisms. The training sample propagates forward\n",
    "through the network and the output error is back propagated and the error is minimized\n",
    "using the gradient descent method, which will calculate a loss function for all the weights\n",
    "in the network\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"mlp.png\", width = 600/>\n",
    "</div>\n",
    "\n",
    "Let’s see an MLP algorithm in action from scikit-learn library on a classification\n",
    "problem. We’ll be using the digits dataset available as part of scikit-learn dataset, which\n",
    "is made up of 1797 samples (which is a subset of MNIST dataset) handwritten grayscale\n",
    "digit of 8x8 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_digits\n",
    "np.random.seed(seed=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1797 samples\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "digits = load_digits()\n",
    "print('We have %d samples'%len(digits.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAEwCAYAAACXPCS8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3db2xVVb7/8U+HgpHwp0WpggdaCqbUFvsHFB8YW41FLjGWfxoQc/kbfOBNCsYEk/sAGhNBE0MrklzHB0jGXBq9UUpQiPwRMCi3odBmCA7BsVRaHRQuqFCY0mN/D/i1gwr7u7rPPt279P1KJhmzVtf5nnXW3mexzzmfndLZ2SkAAAD0zJ/CLgAAAKAvYhMFAADgA5soAAAAH9hEAQAA+MAmCgAAwAc2UQAAAD6k9qTznXfe2ZmVlZXQA54/f97s09LS4tk+bNgwc4xYLGb2GTBggNnHUl9ff7azs3OkFMz8uDhx4oRnezweN8cYPXq02SctLc25ppu5fn6k3pujX375xbP973//uznG7bffbvbJyclxrulmgl5D//jHP8w+ra2tnu2DBg0yx7jvvvvMPn31GLOOoaamJnOMCRMmBFWOp2QcY9Y5RpJuu+02z/beeJ1c9dXztMsxFoRkrKEzZ86Yfaw5uHDhgjnG5cuXzT7WeWjSpEnmGA0NDb+Zoy492kT985//VH19fU/+RJKUmZmpU6dOSZI++OADs/+qVas828vKyswx1q1bZ/ZJT083+1hSUlKau/6/3/mRfjtHltLSUs92l4VXWVlp9ikvL3eqx8v18yMFs4Zc7Nu3z7N95syZ5hiFhYUJP46LoNfQa6+9ZvZ9+eWXPdvvuecec4y9e/eafYI+xrKysnT27Fk1Nzd7/ckN9WQNWcfQokWLzDG2bt3q9FiJCuoYk/41R9Y5RrI3Se+++66vGpIhjDUUxHn68OHDTo+VqGScp6uqqsz+1hy4HEONjY1mnyFDhni2f/bZZ+YY6enpN1wwPdpEXbx4UX7COVNSUnr8N32R3/mRmCML82PrL3PU3NzMGvLAGrKxhrxxnnbHd6IAAAB8YBMFAADgQyCbqJ07dyonJ0cTJkxw+i5Sf7NkyRJlZGQoPz8/7FIi6fTp03r00UeVm5urvLw8VVdXh11SpFy5ckUPPvigCgoKlJeXp9WrV4ddUiTF43EVFRXpySefDLuUSMrKytKkSZNUWFioKVOmhF1O5Fy4cEFz587VxIkTlZubqy+//DLskiLlxIkTKiws7P7fsGHDnL73dKvr0XeibiQej+uFF17Qrl27FIvF9MADD+ipp57qtV8V9AWLFi3Sf/zHf+jf//3fwy4lklJTU/XGG2+ouLhYv/zyiyZPnqyysjLW0P932223ae/evRoyZIiuXr2qhx9+WP/2b/+mhx56KOzSIqW6ulq5ubn6+eefwy4lsj777DPdeeedYZcRSRUVFZo+fbr+53/+R+3t7Wprawu7pEjJyclRQ0ODpGvv+/fcc49mzZoVclXhS/hKVF1dnSZMmKDs7GwNGjRI8+bNU21tbRC13TIeeeQRjRgxIuwyImvUqFEqLi6WJA0dOlS5ubnmT/D7k5SUlO5fl1y9elVXr17tl1/g9NLS0qKPP/5Yy5YtC7sU9EE///yzDhw4oKVLl0q6FvERRMTLrWrPnj0aP368MjMzwy4ldAlvolpbWzVmzJju/47FYrwBwrdTp07p6NGjmjp1atilREo8HldhYaEyMjJUVlbG/PzOihUr9Prrr+tPf+JrnjeTkpKiadOmafLkyfrzn/8cdjmR8s0332jkyJFavHixioqKtGzZMl26dCnssiKrpqZG8+fPD7uMSEj447wb/QzS61/JVgaUZAfZuQR2ulz5ef/99z3bn376aXOMMFj/Qtq/f785hksuRhA5UT1x8eJFzZkzR1VVVZ6Bql2XlL08+uijnu3Dhw83x+hJLlWyDRgwQA0NDbpw4YJmzZqlY8eO3fA7di+//LK5riXp7bff9mx//vnnzTFccmQef/xxs0+itm/froyMDE2ePDmQ3C7JzjhyyRCLmoMHD2r06NH64YcfVFZWpokTJ+qRRx65YV+XtW+dZzZv3myO4XIlozeOw46ODh05ckQbNmzQ1KlTVVFRoXXr1umVV165YX+XT1us+emr321sb2/Xtm3btHbt2oTHst7LXL5zFUQeVSJXHRP+Z1ssFtPp06e7/7ulpcUpDRu43tWrVzVnzhwtWLBAs2fPDrucyEpLS1Npaal27twZdimRcfDgQW3btk1ZWVmaN2+e9u7dq+eeey7ssiKn67yckZGhWbNmqa6uLuSKoiMWiykWi3Vf4Z07d66OHDkSclXRtGPHDhUXF+uuu+4Ku5RISHgT9cADD+jkyZNqampSe3u7ampq9NRTTwVRG/qJzs5OLV26VLm5uXrxxRfDLidyfvzxx+5/SV2+fFm7d+/WxIkTQ64qOtauXauWlhadOnVKNTU1euyxx/Tee++FXVakXLp0qftWSJcuXdKnn37Kr4Wvc/fdd2vMmDHdt2rZs2cPP2y5iS1btvBR3nUS/jgvNTVVb731lp544gnF43EtWbJEeXl5QdR2y5g/f7727duns2fPKhaLqbKysvsLjLh2JeEvf/lL98+vJenVV1/VjBkzQq4sGr7//nstXLhQ8Xhcv/76q5555hl+xo8eOXPmTPcvqTo6OvTss89q+vTpIVcVLRs2bNCCBQvU3t6u7Oxsbdq0KeySIqetrU27du0yvw7QnyS8iZKkGTNm8IbnYcuWLWGXEGkPP/yw79tU9Af333+/jh49GnYZfUJpaanTfd/6m+zsbKd7jPVnhYWFvXavur5q8ODBOnfuXNhlRAo/ZQEAAPCBTRQAAIAPbKIAAAB8COQ7US66MmWsDChJ+vvf/+7Znp2dbY5RVlbmXNPNhJET5ZKBFEQWTl/MuemydetWs09BQYFn+8yZM80xKisrnWuKiuXLlztlsU2ePNmzfdy4ceYYvZEBlQxWZoxk50StWLHCHCOIfKOsrKyEx/DDJTenubnZs90li83l+2vJzPjxK4iMJ5dzUF/mcoxY1qxZY/ZxOc6Cyo+7Ea5EAQAA+NCjK1FDhgzxdc+u/hK+6Xd+JLfk3luB3zlifmyxWCzgaqIpMzOTNeSB85CNNeSN87S7Hm2icnJyzJ+AutwK4lblMj/9HXPkzWV+vvnmm16qJpqidDueKOIYs7GGvLGG3PFxHgAAgA9sogAAAHxgEwUAAOADmygAAAAf2EQBAAD4EHjY5vnz5z3bi4uLzTFcwjQtVphgWKqqqjzbXcLFfvrpp4Tr6Ms3aXUJcbNCCl3GKC8vdy2pV1nHh8uv96zQW5cgTetYl6T09HSzT2+zgjQl+9dbixYtMsdwWWNWUKTL+SAZXEI+rRsau5ynXEJ/wwjTtLgEtlqBv3058FiyAyyDCLi03i9dWQHNLsfzzXAlCgAAwAc2UQAAAD6wiQIAAPCBTRQAAIAPbKIAAAB8YBMFAADgA5soAAAAH3o9J6qsrCzoh/RVhxROho2VHeOSVxFE3S45J2GxanPJDrFyQVy45AlFkUvO2v/93/95trvkRLn02b17t2d7Mo7B2tpaz/aVK1eaYyxcuDDhOqqrq80+mzZtSvhxksHl+LFygBoaGswxXF4Li0seV9Bczp9W1pbLeWzmzJkJP06yWI/r8voHkSXlslaTmYvIlSgAAAAf2EQBAAD4wCYKAADABzZRAAAAPrCJAgAA8IFNFAAAgA9sogAAAHxgEwUAAOBD4GGbVnhefX19wo/hEqR5+PBhs88zzzyTcC19lUsQWmFhYS9U8kdr1qzxbHcJMbS4BLSlpaUl/DhRZR2nVkimJD3//PNmn9dee82zfd26deYYPTV8+PCE2iVp8+bNnu0ux48LlzDFqEpmgOH1Tp061SuP0xMuAZf79+/3bHcJ7HQJIz169Khne7LO49YcuJxjU1JSEh6jt9bhzXAlCgAAwAc2UQAAAD6wiQIAAPCBTRQAAIAPbKIAAAB8YBMFAADgA5soAAAAH9hEAQAA+BB42GZ2drZnu0sI5gcffJBQu6tVq1YFMg6CtWjRIs/2ffv2mWM0NjZ6truEHJaXl5t9Fi9enPAYQXv55ZfNPo8//rhnu0ug7a5du8w+YQTaWuF7LiGHVpimS8DfwoULzT5RDXStra01+1ihpVZorqsoBpJa5yjJDsp0Cex0CRq1AinDCk1esWKF2cdaQyUlJUGVkzRciQIAAPCBTRQAAIAPbKIAAAB8YBMFAADgA5soAAAAH9hEAQAA+MAmCgAAwIdez4l67bXXzDGs/KYpU6aYY9TX15t9osglN8bKHnLJeHHJWnLJQkkGK9fEyvBx6eOSYeMyj1bWSxg5Uenp6Waf5cuXJ/w4LhlQb7/9dsKPEwbrOPzpp5/MMcI6foLw2WefmX2qq6sTfhyXLC2XTK7e5vLaWhlP7777rjmGy3OPYo6W5PYes3nzZs/2qOaoXY8rUQAAAD6wiQIAAPCBTRQAAIAPbKIAAAB8YBMFAADgA5soAAAAH9hEAQAA+MAmCgAAwIeUzs5O984pKT9Kak5eOX1SZmdn50iJ+bmJ7vmRmKObYA15Y368cYzZWEPeWEO238xRlx5togAAAHANH+cBAAD40KN75915552d1r3CLKdPnzb7XLhwwbP9jjvuMMe46667zD4DBgww+1jq6+vPdl3iC2J+vv76a7NPPB73bM/JyUmohiBdPz9SMHNkPX9J+u677zzbz507Z44xZMgQs8+ECRPMPpag11AQ/vrXv5p9XI4fay26jBH0/FjnF0k6c+aMZ7vL6x7E+cWFn2Osvb3ds916/pJ9DLk8f5d7o1nn+8GDB5tjhHGMWeegH374wRxj0qRJZp+g38cktzmyjiOXNWSdyy9fvmyO4cKax0GDBplj/H6OuvTo47yhQ4d2Xrx40bl/l8zMzO6bMa5YscLsv3XrVs92l5s/ujxOEDc3TElJqe/s7Jwi+Z8f6V9z5HIzSWvxutz4sbdcPz9SMGvI5U3QusFwUDf/tNaqi6DXUBBc3mRcjh9rLbqMcf38TJkypfPs2bNqbu751zW65sflxtLr16/3bHd53Xvr5qlBHWOxWEyff/65JKmqqsrsbx1DLs/f5Xxnne+tG5ZLwa8hF9Y5yGWOXR4r6Pcxyf8auvvuu/Vf//VfkuxjSLLP5Y2NjT2u4Uaampo8213Od7+foy49uhJ18eJF+fkOVUpKSo//pi/yOz8Sc2Rhfmz9ZY6am5tZQx44xmysIW+sIXd8JwoAAMAHNlEAAAA+sIkCAADwIZBN1Pr165WXl6f8/HzNnz9fV65cCWLYW0Z1dbXy8/OVl5fn9GXC/mbnzp3KycnRhAkTtG7durDLiZwlS5YoIyND+fn5YZcSSadPn9ajjz6q3Nxc5eXlqbq6OuySIufKlSt68MEHVVBQoLy8PK1evTrskiIpHo+rqKhITz75ZNilRFJWVpYmTZqkwsJCTZnyh+9Y90sJb6JaW1v15ptv6vDhwzp27Jji8bhqamqCqO2WcOzYMb3zzjuqq6tTY2Ojtm/frpMnT4ZdVmTE43G98MIL2rFjh44fP64tW7bo+PHjYZcVKYsWLdLOnTvDLiOyUlNT9cYbb+irr77SoUOHtHHjRtbQ79x2223au3evGhsb1dDQoJ07d+rQoUNhlxU51dXVys3NDbuMSPvss8/U0NCgw4cPh11KJARyJaqjo0OXL19WR0eH2traNHr06CCGvSV89dVXeuihhzR48GClpqaqpKREH330UdhlRUZdXZ0mTJig7OxsDRo0SPPmzXP6CXp/8sgjj2jEiBFhlxFZo0aNUnFxsSRp6NChys3NVWtra8hVRUtKSkp37tnVq1d19erVfvlLKi8tLS36+OOPtWzZsrBLQR/So4iDG7nnnnv00ksvaezYsbr99ts1bdo0TZs27ab9GxoaEn1Ip4wfl6yk3shTys/P13/+53/q3Llzuv322/XJJ5/c9DKoa4aNxeXkWFBQYPYJ4rWytLa2asyYMd3/HYvF9L//+7+ef+OSE2bNo8vHGS7rzOrjUmsYrPlxydBx6WPlwASdpXTq1CkdPXpUU6dOvWmfhQsXmuNYdbmsDZesut4Uj8c1efJkff3113rhhRduOEdduUQu50br+bnkubl89Gq9Fi45US5WrFih119/Xb/88ovZ1+W5WWskqMDP3jzGUlJSNG3aNKWkpOj555/X8uXL/9Bn06ZNkqT9+/eb4w0fPtyz3eU87ZLnl8xw1YSvRJ0/f161tbVqamrSd999p0uXLum9994LorZbQm5urlatWqWysjJNnz5dBQUFSk1NeO96y7hRFgn/QoYfFy9e1Jw5c1RVVaVhw4aFXU7kDBgwQA0NDWppaVFdXZ2OHTsWdkmRsX37dmVkZGjy5MlhlxJpBw8e1JEjR7Rjxw5t3LhRBw4cCLuk0CW8idq9e7fGjRunkSNHauDAgZo9e7a++OKLIGq7ZSxdulRHjhzRgQMHNGLECN17771hlxQZsVjsN7cCamlp4eNg9NjVq1c1Z84cLViwQLNnzw67nEhLS0tTaWkp37O7zsGDB7Vt2zZlZWVp3rx52rt3r5577rmwy4qcrnNzRkaGZs2apbq6upArCl/Cm6ixY8fq0KFDamtrU2dnp/bs2cMX836n6x5J3377rT788EPNnz8/5Iqi44EHHtDJkyfV1NSk9vZ21dTU6Kmnngq7LPQhnZ2dWrp0qXJzc/Xiiy+GXU4k/fjjj90f+1y+fFm7d+/WxIkTQ64qOtauXauWlhadOnVKNTU1euyxx/hE5XcuXbrU/VHnpUuX9Omnn/KLYQXwnaipU6dq7ty5Ki4uVmpqqoqKim74OWl/NmfOHJ07d04DBw7Uxo0blZ6eHnZJkZGamqq33npLTzzxhOLxuJYsWaK8vLywy4qU+fPna9++fTp79qxisZgqKyu1dOnSsMuKjIMHD+ovf/lL90+vJenVV1/VjBkzQq4sOr7//nstXLhQ8Xhcv/76q5555hl+xo8eOXPmjGbNmiXp2o/Jnn32WU2fPj3kqsIXyJdzKisrVVlZGcRQt6Sum3rixmbMmMEbnoctW7aEXUKkPfzww77vN9hf3H///Tp69GjYZfQJpaWlTl9W7m+ys7MDuyHwrYTEcgAAAB/YRAEAAPjQ67+1d8n0sDIdXDJaXLIxrCyU3r6k65I9IkklJSWe7S6ZGL2RkRW0rgwblywtKwtozZo15hgur0dvZGklQ0VFRcJjWOtQSm4+i19BHB8zZ840x4haTpSLrnOey7q2zsMux5iVEyS5zXVvc3ltrfPH1q1bzTFc1qr1PuXyOEHqeo93WUPWfsBlnoPOmusprkQBAAD40KMrUUOGDPEVhJiZmdnjv+mL/M6PdC35vT/wO0exWCwJ1URPImuovxxnmZmZnIc8cJ62sYa8+V1Dd9xxRxKqibYebaJycnK46aAHl/npqx//BMVljro+tuuPOMZs/Xl9uGAN2VhD3lzWkMvHtf0BH+cBAAD4wCYKAADABzZRAAAAPrCJAgAA8IFNFAAAgA+9Hra5aNEis09RUZFnu8svK1wCuMIIAgziMa3wNJdwOtdgzzAEEZ7mss56o45ksF47l4C65ubmoMqJHOv84BL4a732/LormBBHl18rh3Gerqqq8mzfvHmzOcb69es9212e108//WT2cVnPYXA5x1h9XJ5b2MciV6IAAAB8YBMFAADgA5soAAAAH9hEAQAA+MAmCgAAwAc2UQAAAD6wiQIAAPCh13Oigsgn2r9/v9mnqanJ7BNG/oiVP1NQUGCOkZ6e7tleUVFhjuGSz2LlbyRr/lxq68+s18UlNyUzM9Oz3SXjJar5NNa6DOLu8y7z43Kui2oWmQsrS8llfbhkmgWRR9VTQWQPvfvuu57t1vy5snIVk8V67YJ4f1i8eHHCYyQbV6IAAAB8YBMFAADgA5soAAAAH9hEAQAA+MAmCgAAwAc2UQAAAD6wiQIAAPCBTRQAAIAPgYdtWkGJjz76qDnG6tWrPdtdgtBmzpxp9rFC3MII43QJmrT6BBWCaIWpJSsEL4j6f/rpJ892lyBEl9ciiODGnrLmZ9++feYYtbW1nu0ux48VJigFFygYJJfj2npuw4cPN8foy0GaLqx5dDl+XI51az2XlpaaY/SUdVy7nD+s86N1jpLsUFxJKi8vN/skg7W+Fy1aZI5hnYdcBLXO/OJKFAAAgA9sogAAAHxgEwUAAOADmygAAAAf2EQBAAD4wCYKAADABzZRAAAAPrCJAgAA8CHwsE0rgM0lpM4KeXQJ2ywqKjL7WIF6YQQpurCCw6z5k9yCEpMVpmmxQtxKSkrMMdavX+/Z/tFHHyVch5TcELdkcjkOLX01TNLl+KiurvZsD+I8JgUTWOiHFRa5f/9+c4zz5897trsErboETrqc74NmvS4u509rjtPT080xkhEkGpQg1tDChQs92wsKCswxwj4HcyUKAADABzZRAAAAPrCJAgAA8IFNFAAAgA9sogAAAHxgEwUAAOADmygAAAAfAs+JsvI1XHIvrPwMl4yW8vJys49Ljktvc6mpoaHBs93K75Ckffv2mX3Czt+4GZf8KmserTmU3LJg+irrtXXJZ2lsbDT7WGsxjKwpl+wlK5vI5dhwWadBnC/9sF4XK2ctKC7n6WRlZSWbdQ5yeR+L8nO3zqFWBpRk54SFlVXYE1yJAgAA8IFNFAAAgA9sogAAAHxgEwUAAOADmygAAAAf2EQBAAD4wCYKAADABzZRAAAAPqR0dna6d05J+VFSc/LK6ZMyOzs7R0rMz010z4/EHN0Ea8gb8+ONY8zGGvLGGrL9Zo669GgTBQAAgGv4OA8AAMAHNlEAAAA+9OgGxHfeeWdnVlZWkkr5l++++86z/YcffjDHmDRpktlnwIABzjXdTH19/dmuz0mDmJ94PG72OXPmjGe7y/y43Pg1iNf6+vmRgpkj6+awknT77bd7tp87d84cY+jQoWafMWPGmH0sQa8hl+dmHWMuNbjMTxCCnp+2tjazj7XGBg0aZI7hMj933XWX2ceSjGPMRXt7u2f7X//610AexzqXu7wWQa8h6/iRpO+//96zffz48eYYvXWDbj9ryHqv+sc//mE+7s8//+zZ7nKsuryPZ2dne7YPGzbMHOP3c9SlR9+JGjp0aOfFixed+3fJzMx0euPrsmbNGs/2qqoqcwyXxwtigaakpNR3dnZOkfzPj/SvObLuri7Zz99lfmbOnGn2effdd80+luvnRwpmDbnc2bywsNCz3eW5lZaWmn1c5toS9BpyeW7WMRbU/ATh+vmZMmVK59mzZ9Xc3PPvvHbNj3X3ecleYy5vwi7zs2LFCrOPJahjTOrZudrqN27cOF81/F5TU5Nnu8trEfQaso4fSaqsrPRs37p1qzlGeXm5a2kJCWoNjR07tvv4eu2118z+u3bt8mw/cuSIOYbL+/gHH3zg2f7444+bY/x+jrr06ErUxYsX5eeL6CkpKT3+m77I7/xIzJGF+bH1lzlqbm5mDXlgDdlYQ944T7vjO1EAAAA+sIkCAADwIZBN1IULFzR37lxNnDhRubm5+vLLL4MY9pZw4sQJFRYWdv9v2LBhgXyP5layfv165eXlKT8/X/Pnz9eVK1fCLilyqqurlZ+fr7y8PNbPDezcuVM5OTmaMGGC1q1bF3Y5kbNkyRJlZGQoPz8/7FIi6fTp03r00UeVm5urvLw8VVdXh11S5Fy5ckUPPvigCgoKlJeXp9WrV4ddUiQEsomqqKjQ9OnT9be//U2NjY3Kzc0NYthbQk5OjhoaGtTQ0KD6+noNHjxYs2bNCrusyGhtbdWbb76pw4cP69ixY4rH46qpqQm7rEg5duyY3nnnHdXV1amxsVHbt2/XyZMnwy4rMuLxuF544QXt2LFDx48f15YtW3T8+PGwy4qURYsWaefOnWGXEVmpqal644039NVXX+nQoUPauHEja+h3brvtNu3du1eNjY1qaGjQzp07dejQobDLCl3Cm6iff/5ZBw4c0NKlSyVd+7lpb/0ss6/Zs2ePxo8fr8zMzLBLiZSOjg5dvnxZHR0damtr0+jRo8MuKVK++uorPfTQQxo8eLBSU1NVUlKijz76KOyyIqOurk4TJkxQdna2Bg0apHnz5qm2tjbssiLlkUce0YgRI8IuI7JGjRql4uJiSdeiKXJzc9Xa2hpyVdGSkpKiIUOGSJKuXr2qq1ev9ssvkv9ewpuob775RiNHjtTixYtVVFSkZcuW6dKlS0HUdsupqanR/Pnzwy4jUu655x699NJLGjt2rEaNGqXhw4dr2rRpYZcVKfn5+Tpw4IDOnTuntrY2ffLJJzp9+nTYZUVGa2vrb/K6YrEYb4Dw7dSpUzp69KimTp0adimRE4/HVVhYqIyMDJWVlTFH6mHEwY10dHToyJEj2rBhg6ZOnaqKigqtW7dOr7zyiu8x9+3b59nucqUralfD2tvbtW3bNq1du/amfRoaGpwykKx8Fpfnbs1xbzl//rxqa2vV1NSktLQ0Pf3003rvvff03HPP3fRvXJ6flQXkMoZLVpKV8xNE6GFubq5WrVqlsrIyDRkyRAUFBUpNvfmh65I/Y2XkRCknynKjn2J7/QvZJeOnsbExoXZJTlfDrLy23gjN9Ksn2X99xcWLFzVnzhxVVVXdNIDxwoULgWQ8uWT1Re3etgMGDFBDQ4MuXLigWbNm6dixY3/4nt0333wjSaqvrzfHKysrS6hdsrOmJGnVqlWe7S613kzCV6JisZhisVj3jnTu3LlOAVn9zY4dO1RcXBxIQvGtZPfu3Ro3bpxGjhypgQMHavbs2friiy/CLityli5dqiNHjujAgQMaMWKE7r333rBLioxYLPabK3MtLS18JIweu3r1qubMmaMFCxZo9uzZYZcTaWlpaSotLeV7dgpgE3X33XdrzJgxOnHihKRr3/u57777Ei7sVrNlyxY+yruBsWPH6tChQ2pra1NnZ6f27NnDDxNuoOtWPt9++60+/PBD1tJ1HnjgAZ08eVJNTU1qb29XTU2NnnrqqbDLQh/S2aHz6mYAABIkSURBVNmppUuXKjc3Vy+++GLY5UTSjz/+2H1HjcuXL2v37t2aOHFiyFWFL+GP8yRpw4YNWrBggdrb25Wdna1NmzYFMewto62tTbt27dLbb78ddimRM3XqVM2dO1fFxcVKTU1VUVGRli9fHnZZkTNnzhydO3dOAwcO1MaNG5Wenh52SZGRmpqqt956S0888YTi8biWLFmivLy8sMuKlPnz52vfvn06e/asYrGYKisru38MBOngwYP6y1/+okmTJnXfMurVV1/VjBkzQq4sOr7//nstXLhQ8Xhcv/76q5555hk9+eSTYZcVukA2UYWFhTp8+HAQQ92SBg8e7HRT2P6qsrLSvM9Uf/f555+HXUKkzZgxgzc8D1u2bAm7hEh7+OGHI/f9o6i5//77dfTo0bDLiBwSywEAAHxgEwUAAOADmygAAAAfAvlOVE9Y+T2StH//fs/29evXB1VOpDQ3N3d/qdGLNYdBZE1FmUu+inV/OZf8HZcsqSjm+ASxhjZv3myO4ZK3FLX52bdvn1N+U0VFhWe7y3N3eR2i6MKFC07naZc5sJSUlJh9oraG0tLSnF5bK2stiOPUdZzeNHnyZElu+U2WrswpL++//77Z5/nnn0+4lpvhShQAAIAPPboSNWTIEF/3yukv94rzOz/Stbyt/oA15C2RNdRf5igzM9PXHI0aNSoJ1URPImto7NixAVcTTX7XUH85xjhPu+vRJionJ4coAw8u89Pfb4zKGvLG/Nisj6KjckujsLisoa7QxP6qL3+doTdwHnLHx3kAAAA+sIkCAADwgU0UAACAD2yiAAAAfGATBQAA4EMkwzYtLmGLUVVeXu7Z7vITUesXflu3bjXHcJlD6xcsYYXguYTLWXO0cOFCcwwrLC+qVqxYYfaxfsHm8tq6PI7LWowiK6zVRXNzcwCVJIe1tleuXNk7hfRRLmvf+gVkY2OjOUbUgkaDZoVpjh8/3hyjuLjY7LN8+XLnmnqKK1EAAAA+sIkCAADwgU0UAACAD2yiAAAAfGATBQAA4AObKAAAAB/YRAEAAPjQ6zlRQdw9fNy4cWafgoICs09lZaVnu5XplAxFRUW98jibN282+1g5UVbWULK4ZFxZr/+iRYvMMdLS0lxLihSXuoN47VxeBysXziXzq6dKS0sTHsM6T7nMcUlJidnHymtas2aNOYYfVs6RyxxaWVpBnGOiymXdWnPssj766jnIVXZ2tme7y3v9yy+/bPZJT093rqmnuBIFAADgA5soAAAAH9hEAQAA+MAmCgAAwAc2UQAAAD6wiQIAAPCBTRQAAIAPbKIAAAB86PWwzSDC4yoqKhIvxGGcZIRtWiF+q1evNsewghJdAuxcwiZdwhSjypojl+cWVphoX7F48WKzj3W8b926NaBq3A0fPtzsY9VtBU1KbsHCWVlZZp8wuIRJBlF7VJ+/xQqRlewgVZcw0v6urKzM7LNq1Sqzz9NPPx1EOTfElSgAAAAf2EQBAAD4wCYKAADABzZRAAAAPrCJAgAA8IFNFAAAgA9sogAAAHxgEwUAAOBDr4dtugQYBhHyuGLFCrNPdXW1Z7tLaGVPpaWleba7hJFaQZkuIX9BhJ4mi1W/S0CfNUYyXtuocHn9XcICLU1NTWaf2tpaz/YwXgeXoNkgQkBdXoe+HGgbRFDm/v37zT7WGgkjsDOI1+3o0aOB9LFqcQlOTYbXXnvN7HP+/HnP9vfff98cw+U4SyauRAEAAPjAJgoAAMAHNlEAAAA+sIkCAADwgU0UAACAD2yiAAAAfGATBQAA4EOv50S5ZFZYGU8uGUdWBpQklZeXe7aHkT/iwsrFKC0t7Z1CksTK0nLJBbHmwGV99FXvvvuu2WflypUJP05BQYHZxzrGrNc6GVwy5KwcLZe8O5fXIYznHxTrGCspKTHHcMkri2JOlMsasrLGXNaQi956nJ5at26d2cc6lz/++OPmGG+//bZzTcnAlSgAAAAf2EQBAAD4wCYKAADABzZRAAAAPrCJAgAA8IFNFAAAgA9sogAAAHxgEwUAAOBDSmdnp3vnlJQfJTUnr5w+KbOzs3OkxPzcRPf8SMzRTbCGvDE/3jjGbKwhb6wh22/mqEuPNlEAAAC4ho/zAAAAfOjRvfPuvPPOzijcT+7EiRNmn3Hjxpl9Bg0alHAt9fX1Z7su8QUxPy7PbejQoQk9hiQNGDDA7HPXXXcl/DjXz48UzBzF43GzjzWPLmOMHz/e7DN48GCzjyXoNeTiu+++82w/d+5cII+Tk5Pj2e5yDAY9P9a92CSpo6PDs93l2AjiOHWRjGPs9OnTZp9ffvnFs/2OO+4wxwjiHOMijDVkzc/tt99ujjF69GizT9DnICmYOWpvbzf7fP31157tLveWdJmjIPx+jrr0aBP1z3/+U/X19T1+8MzMTKdF58rlBrsuN/8M4s0qJSWl+3Njv/Mj/WuOXJ5bEDcYdlmcLjfZtFw/P1IwayiIGxC7jPHf//3fZh+XG2pbgl5DLqybeLscPy62bdvm2e5yDF4/P1lZWTp79qyam3v+dY2u+Vm0aJHZ11ofLsdGb90IPKhjTPrXHLk8P+vGti7zHMQ5xkUYa8iaH5dzh3Wcuo5jScZ52uVcNHPmzITaJbc5CsLv56hLjzZRFy9elJ/vUKWkpPT4b/oiv/MjMUcW5sfWX+aoubmZNeSBNWRjDXnjPO2O70QBAAD4wCYKAADAh0A2UfF4XEVFRXryySeDGO6Wk5WVpUmTJqmwsFBTpkwJu5xIunDhgubOnauJEycqNzdXX375ZdglRcaJEydUWFjY/b9hw4apqqoq7LIiZf369crLy1N+fr7mz5+vK1euhF1S5FRXVys/P195eXmsnxvYuXOncnJyNGHCBK1bty7sciKJNfRHgWyiqqurlZubG8RQt6zPPvtMDQ0NOnz4cNilRFJFRYWmT5+uv/3tb2psbGQ9XScnJ0cNDQ1qaGhQfX29Bg8erFmzZoVdVmS0trbqzTff1OHDh3Xs2DHF43HV1NSEXVakHDt2TO+8847q6urU2Nio7du36+TJk2GXFRnxeFwvvPCCduzYoePHj2vLli06fvx42GVFCmvoxhLeRLW0tOjjjz/WsmXLgqgH/dDPP/+sAwcOaOnSpZKu/ezd5deD/dGePXs0fvx4ZWZmhl1KpHR0dOjy5cvq6OhQW1tbr/3sua/46quv9NBDD2nw4MFKTU1VSUmJPvroo7DLioy6ujpNmDBB2dnZGjRokObNm6fa2tqwy4oU1tCNJbyJWrFihV5//XX96U98vepmUlJSNG3aNE2ePFl//vOfwy4ncr755huNHDlSixcvVlFRkZYtW6ZLly6FXVYk1dTUaP78+WGXESn33HOPXnrpJY0dO1ajRo3S8OHDNW3atLDLipT8/HwdOHBA586dU1tbmz755BOnLKj+orW1VWPGjOn+71gsptbW1hArih7W0I31KOLg97Zv366MjAxNnjzZzMToCSujxiV/IkpXMg4ePKjRo0frhx9+UFlZmSZOnKhHHnnkD/0aGhq0f/9+czyrT3l5uTlGb2XYuOjo6NCRI0e0YcMGTZ06VRUVFVq3bp1eeeWVG/Z3+Sy+sbHRs72goMAcI0prSLoWXrdt2zatXbs24bGsfCaX9bF161azj5UD5DKG5fz586qtrVVTU5PS0tL09NNP67333tNzzz3ne0zrfBZU7p31OEGtwdzcXK1atUplZWUaMmSICgoKlJp689N/Q0ODOaZ1jK1cudIcwyUHqDfCZ2/0c/6b/Vx/37592rx5szmmdY5xee4ufazXKqw15LInsNaQ1S65rQ+XXC+/Erp8dPDgQW3btk1ZWVmaN2+e9u7dm9CJ61bV9dFCRkaGZs2apbq6upAripZYLKZYLKapU6dKkubOnasjR46EXFX07NixQ8XFxb2W8txX7N69W+PGjdPIkSM1cOBAzZ49W1988UXYZUXO0qVLdeTIER04cEAjRozQvffeG3ZJkRGLxX5zVaWlpYWPhG+ANfRHCW2i1q5dq5aWFp06dUo1NTV67LHH9N577wVV2y3h0qVL3fH/ly5d0qeffqr8/PyQq4qWu+++W2PGjOm+VcuePXt03333hVxV9GzZsoWP8m5g7NixOnTokNra2tTZ2ak9e/bww4Qb+OGHHyRJ3377rT788EPW0nUeeOABnTx5Uk1NTWpvb1dNTY2eeuqpsMuKHNbQHyX0cR5sZ86c6f4lVUdHh5599llNnz495KqiZ8OGDVqwYIHa29uVnZ2tTZs2hV1SpLS1tWnXrl16++23wy4lcqZOnaq5c+equLhYqampKioq0vLly8MuK3LmzJmjc+fOaeDAgdq4caPS09PDLikyUlNT9dZbb+mJJ55QPB7XkiVLlJeXF3ZZkcMa+qPANlGlpaWR+p5NVGRnZzt9rtvfFRYWEv/gYfDgwYHdFPhWVFlZqcrKyrDLiLTPP/887BIibcaMGZoxY0bYZUQaa+iP+EkdAACAD2yiAAAAfGATBQAA4EOvf7HcJTti8eLFnu3r1683x3DJElqzZo3ZpzelpaU5JVH3Vi5I1HRl8wTx3Rcri0zqnXyasFi5KS65Ki7zE8W16FK39f1Ol3wrl8exjuUof8/UykFy+S6oy/tBMjN+kslaIy7rw2WdWecyK6stWVy+dD58+HDP9qDmKLI5UQAAAP1Vj65EDRky5KYprl76y32+/M6PdC3srT/wO0fMj62/HGeZmZmchzywhmx+19CoUaOSUE308F7vrkebqJycHH6G7sFlfoK6XURfxRx54xiz9ef14YI1ZLPWUJC3MeuLWEPu+DgPAADABzZRAAAAPrCJAgAA8IFNFAAAgA9sogAAAHzo9bBNl+CvioqKhMdw+XmmFeSVjICuIG5GvH//fs/28vLyhB8jTBcuXEh4jJKSEs/2vhykaf2yyCVI1Ap5dHkNmpubzT5RnGeXkN3CwkLPdpcQ0SBCPcPicu6zQpFduLwWYYRtBvHrvCDWvss6GzduXMKPkwwu70PW679y5UpzjLB/rcuVKAAAAB/YRAEAAPjAJgoAAMAHNlEAAAA+sIkCAADwgU0UAACAD2yiAAAAfAg8J8rKqHHJSbIyWmbOnNmTkm4qjPyRgoICz3aXfB7r+Vs5W5JUVVVl9gmLS/6QxcoOsbK2pOjmbVnHWGVlZe8U4sAl5yZo1jHkctwHkedmZXFFmcscWX1c8ntcMo6sebTeL/oyl7yqMN7HgmJlPrpkQrrkcSVzDXElCgAAwAc2UQAAAD6wiQIAAPCBTRQAAIAPbKIAAAB8YBMFAADgA5soAAAAH9hEAQAA+BB42KYV/JWenm6O8dFHH3m2u4S4RZUVDOYStmk9f5cAu6BCzJIhMzMz4TGswM6gAls3bdrk2Z6MILw1a9Yk1O7CZX76ahCgS91Hjx71bLcCTyW35+4yTl/lcv4IIhi4r86hy7neJXj4Vg4bdeHyXmadE7du3er78bkSBQAA4AObKAAAAB/YRAEAAPjAJgoAAMAHNlEAAAA+sIkCAADwgU0UAACAD2yiAAAAfAg8bNNSXl6ecB+XcLXFixe7lhQpLkGALn0sLoGlYYVtWo/rEsbpElIXBGstRjFsUrID6mpra80x1q9fb/ZJS0tzriko1mO61NTQ0ODZ7nIOCirQNQzW85fs85BLgKHLecg6loMIl/290tJSz/bKykpzDCtM0+XcMHz4cLNPGMdYUKx15hJI6hI2unLlSs/2RAK8uRIFAADgA5soAAAAH9hEAQAA+MAmCgAAwAc2UQAAAD6wiQIAAPCBTRQAAIAPvZ4TFQSXjJbVq1cnv5AkqKqqMvtY2RoVFRXmGFYOSpis3BOX/BkrB8klB8clf6WvZgFZz7+goMAcI6oZWBaXuq3sIZdsor46P5JbFp3LcWhxyaKzcoCSkWdnnR9d8g7T09M920tKSswxgsgEDItL9pI1zy6vrUuWlMv5zC+uRAEAAPjAJgoAAMAHNlEAAAA+sIkCAADwgU0UAACAD2yiAAAAfGATBQAA4AObKAAAAB9SOjs73TunpPwoqTl55fRJmZ2dnSMl5ucmuudHYo5ugjXkjfnxxjFmYw15Yw3ZfjNHXXq0iQIAAMA1fJwHAADgA5soAAAAH9hEAQAA+MAmCgAAwAc2UQAAAD6wiQIAAPCBTRQAAIAPbKIAAAB8YBMFAADgw/8DMM5p9KOAcmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the first 32 samples to get a sense of the data\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05,\n",
    "wspace=0.05)\n",
    "for i in range(32):\n",
    "    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.gray_r)\n",
    "    ax.text(0, 1, str(digits.target[i]), bbox=dict(facecolor='white'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Parameters for scikit-learn MLP\n",
    "`hidden_layer_sizes` – You have to provide the number of hidden layers and neurons\n",
    "for each hidden layer. For example, hidden_layer_sizes – (5,3,3) means there are 3\n",
    "hidden layers and the number of neurons for layer 1 is 5, layer 2 is 3, and for layer 3 is 3\n",
    "respectively. The default value is (100,) that is, 1 hidden layer with 100 neurons.\n",
    "\n",
    "`Activation` – This is the activation function for hidden layer, and there are four\n",
    "activation functions available for use, default is ‘relu’.\n",
    "   - relu: The rectified linear unit function, returns f(x) = max(0, x).\n",
    "   - logistic: The logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "   - identity: No-op activation, useful to implement linear bottleneck,\n",
    "returns f(x) = x.\n",
    "   - tanh: The hyperbolic tan function, returns f(x) = tanh(x).\n",
    "\n",
    "`solver` – This is for weight optimization’ there are three options available, the default\n",
    "being ‘adam’.\n",
    "   - adam: Stochastic gradient-based optimizer proposed by Kingma/\n",
    "Diederik/Jimmy Ba, which works well for large dataset.\n",
    "   - lbfgs: Belongs to family of quasi-Newton methods, works well for\n",
    "small datasets.\n",
    "   - sgd: Stochastic gradient descent.\n",
    "\n",
    "`max_iter` – This is the maximum number of iterations for solver to converge, default\n",
    "is 200.\n",
    "\n",
    "`learning_rate_init` – This is the initial learning rate to control step size for updating\n",
    "the weights (only applicable for solvers sgd/adam), default is 0.001.\n",
    "\n",
    "**Side Note :  It is recommended to scale or normalize your data before modeling as MLP is sensitive to\n",
    "feature scaling**\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"image-class.gif\", width = 1000/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 1437\n",
      "Number of samples in test set: 360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=100, max_iter=100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data to training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.\n",
    "target, test_size=0.2, random_state=2017)\n",
    "print('Number of samples in training set: %d' %(len(y_train)))\n",
    "print('Number of samples in test set: %d' %(len(y_test)))\n",
    "# Standardise data, and fit only to the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "# Apply the transformations to the data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Initialize ANN classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100), activation='logistic', max_iter = 100)\n",
    "# Train the classifier with the traning data\n",
    "mlp.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.990953\n",
      "Test set score: 0.986111\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: %f\" % mlp.score(X_train_scaled, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results from the test data\n",
    "X_test_predicted = mlp.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAEwCAYAAACXPCS8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAc7klEQVR4nO3dT2zex3kn8OFq14ANGbRi2UacGGRiI94FalCKctgbKezVAJkCSdETqYuzN5EIsMlNYi+NDqnEo3KhdGqRLCCqlx7sQuRdgkSgSOCsY0lIIEeODImRIMNeaLmHgIKjRr/n4bzz/qH7+QABUsxk3vnNOzPv05f29x3b3t4uAADszn8a9gQAAPYiRRQAQAVFFABABUUUAEAFRRQAQAVFFABAhf+8m84HDx7cnpyc7OkF7927F/a5ceNGZ/szzzwTjpGZ53PPPRf2iVy5cuXO9vb2S6W0WZ9Hjx6Ffe7cudPZ/umnn4ZjvPzyy2Gf1utTSps1+uSTT8I+0Ro9ePAgHOPFF18M+/T6LKW030MPHz4M+0RnLGNUz9itW7c62z/66KPwNffv39/ZnjljmbP89a9/vbP9lVdeCcfoxxnL3NO3b9/ubM+csX379oV9Dh06FPaJtD5j9+/fD/tEZ+zZZ58Nx3jjjTeyU+pJP/ZQRrTPonu8lNwaRZ8Zmbv+yTXasasianJysly+fHk3/5N/5+LFi2Gf+fn5cB6Rc+fOhX1aHM6xsbGbO/+9xfpkLq/o2a5duxaOsbi4GPZpvT6ltFmjzHsb9dnY2AjHePvtt5vMJdJ6D2Xe/4WFhZ5eo5TRPWMnT57sbF9eXg5f88iRI53tmTXe2toK+/zwhz/sbM+c036cscw9ffr06c72zBmLitVSSs/PUkr7M7a+vh72ic5Y5mysra0lZ9SbfuyhjGifra6uhmNk1ii6qzL34ZNrtMOf8wAAKiiiAAAqKKIAACooogAAKiiiAAAqKKIAACooogAAKuwqJ6qFKFsk44UXXgj7zM3NhX1aBA62dubMmbBPJucmknn2TBZKP0Svm8nOyfTpdR7DEr13mb3fIkgvymMqZXA5N61fM1rjzDnNrHGLHK0aUc5VZg+dOHGisz3zbCsrK2GfUZR5b2/e/IuxQun2UnJ5ZMPaQ5HM3KNMyMwdk8mrO3bsWGd7Zr8/jW+iAAAqKKIAACooogAAKiiiAAAqKKIAACooogAAKiiiAAAqDDwnKiPKvRjFfCfamZmZ6Wy/d+9ez6+RyVZZWFjo+XX6Idr/mfyZ6NkOHz4cjhFlvJQSZ8X0I+MmypHLzDuTPbOXtThD0Rpl9mGL3MBhyORETUxMdLZn1ifTZ1RzojL3Z9QnM0b0eVFKvM8y2ZNP45soAIAKiigAgAqKKACACoooAIAKiigAgAqKKACACoooAIAKiigAgAoDD9s8c+ZM2CcK+otCzEopZW1tLT2nUTKoINGlpaWBvM6wREGAmbDBxcXFRrNpKwrXm5qaCseI1md5eTkcIxOUOIwgwOiOyQSJRvZ6GOfW1lbPY0RnKHMHz87O9jyPUZUJyoxkPuuGJToDmc+y6I7N7KEWr9ML30QBAFRQRAEAVFBEAQBUUEQBAFRQRAEAVFBEAQBUUEQBAFQYeE5UJl8lysbIZPzsVZlcnfPnz/f8OtPT0z2PMcoOHDjQ2Z7ZQ5lMs5MnT2an1MwLL7zQ2T45ORmOcfHixc72TNbUwsJC2GcYojO0uroajhHlymRyZzL7Z1iuXr3a2T4+Ph6OET3flzkDKnN/RGuYyeqKzvowRXff3NxcOEZ0V2Xu18xn5rVr13oe42l8EwUAUEERBQBQQREFAFBBEQUAUEERBQBQQREFAFBBEQUAUEERBQBQoXnYZhRClgnbjMIko6C4UnJheGtra53twwg6axHQlwnK+7KLgv6i8LVScgFshw8f7mkeNaIzFgVplhKHrWbCBFuE3A3jjGVCQqMQwKNHj4ZjZO6gTDBqP8zMzPQ8RhSEOKphrC20CHzOBNoOa3+0kAmFbhEcPWy+iQIAqKCIAgCooIgCAKigiAIAqKCIAgCooIgCAKigiAIAqKCIAgCo0DxsMwrP29raCseIwiIzQXHLy8thnygIsEUg3W5lwtVu3rzZ2Z5Z470sE3QXvbet1igKfu1H2GZ0xjIhftEaZvb+yspK2CcK180EUu5W9GyZoNUbN270PI/Nzc2wz7DCFKM9lHn+6B7KnNNhhK22kFmf6I7Zy0GapcTnf3V1NRyjRRhvFJpdSn/X2jdRAAAVFFEAABUUUQAAFRRRAAAVFFEAABUUUQAAFRRRAAAVmudERTKZDkePHu35daanp8M+o5jTkVmfkydPdrZncnCiMUop5cyZM2GfYdjY2Aj7zM3NdbZPTEyEYywsLIR9+pFz1Ksom6mU+NkyGVDz8/M9v04/RM+/tLTU82tknr0fGWGtHDp0qLM9kxMW3VVf5pyoFjlzmXt6lNcw2t+Z/R99xmTyuIb9Oe6bKACACoooAIAKiigAgAqKKACACoooAIAKiigAgAqKKACACoooAIAKY9vb2/nOY2N/KKXc7N909qSJ7e3tl0qxPk/xeH1KsUZPYQ91sz7dnLGYPdTNHor92Rrt2FURBQDAn/hzHgBAhV39dt7Bgwe3e/2dmkePHoV9rl+/3tn+4MGDcIyXX3457PPqq6+GfSJXrly5s/MVX2Z9oud///33w9f89NNP8xN8iv3794d93nzzzZ5f54vrU0qbPXTr1q2wz/379zvbM3so41vf+lZn+/PPPx+Osds9FMmsz0cffdTZ/tWvfjUco8X5yWi9Prdv3w77/O53v+tsf/bZZ8MxMuvT4nfP+nHGMlrcZZnnH8Y9HWlxxjJG+Z5++PBhZ/uvf/3r8HWjPZQ5Z5nn37dvX9gn8uQa7dhVETU5OVkuX77c00QyP6gY/Wjp+vp6OMY777wT9sn8CG9kbGzs8d+NM+sTPX/mhz83Nzdzk+tw5MiRsE9mnSNfXJ9S2uyhzPsWzT3zI8YZZ8+e7WzPvJ+73UORzPosLy93tg/q/GS0Xp/MD2tHP1IcFc+lxGtcSpsfKe7HGctocZdFPxReynDu6UiLM5Yxyvd09APKmfc/+iHnzDm7dOlS2KfF/7Py5Brt8Oc8AIAKiigAgAqKKACACoooAIAKiigAgAqKKACACoooAIAKu8qJaiGTC3Ljxo2ex8hkdBw+fLizvUWGy5OiZ8vkWayurna2r62thWNEGR+jLJPzE/XJhO1997vfDftkslAGLdpjGZnzkzmHhw4d6nkurZ07d67nMVpkte11UVZSZh9m9tAoyuyh6enpzvbMHdQiA6pfWtyx0f25srISjpH5LOvnPe2bKACACoooAIAKiigAgAqKKACACoooAIAKiigAgAqKKACACgPPidrY2Aj7nD59urO9Rc5LKaWMj483GWc3otycFrkgmdyMe/fu9fw6/RKtQSZ7KMoFyWTY3L17N+wziqL8nlJKWVhY6Km9lNHdQ9G8BpXxNDExMZDX6YfM+YgyfI4fPx6OkcnFi97PzBitZbL4Ipm8u1HModuRmX8keu8yOVEtcvF64ZsoAIAKiigAgAqKKACACoooAIAKiigAgAqKKACACoooAIAKiigAgAoDD9vMBLBFYYGTk5NtJjMEUXBcJigxChvd2traxYyebnFxsbO9RdjaXxKFbWbC1aKQukyYZOb5Dhw40Nk+OzsbjtFa5ny0CKwd5SDALpkQzJs3b3a2Z4J6M6GwwxKdocx7OzU11dmeucui81NKKaurq53tmbPcWua9jZ7//Pnz4RgtQj37pUXIafTeZc7Z3Nxcz/PohW+iAAAqKKIAACooogAAKiiiAAAqKKIAACooogAAKiiiAAAqDDwnKpO9E/WJspZKGd0sqWju165d6/k1Mjk4KysrPb9Ov0S5H5l8kijjqpWrV692tg8jJ+rixYthn+Xl5c72EydOtJrOyIkyoFrJ3FMtsnZqROcjs0ZRDlKrHLlRztvq0uK9vXv3boOZDEcmvyvKyoru11KGd4Z2+CYKAKCCIgoAoIIiCgCggiIKAKCCIgoAoIIiCgCggiIKAKCCIgoAoMLAwzZbyIRrbW1tDWAmuxeFgC4tLYVjzM/Pd7ZnAjuHHVDWJQrXG1T43vr6etjn3LlzfZ/Hk27cuNHZHu2PUuJA1sz+yDz7MN7LaO6ZMNoobDJzv2TO4czMTNhntx49ehQGfWYCWSPHjh3reYypqamwzyiGbWbuhhaBv1GgaSm5UMt+iPZ3FKRZSimnT5/ubB/F9/5JvokCAKigiAIAqKCIAgCooIgCAKigiAIAqKCIAgCooIgCAKigiAIAqNA8bDMKB1teXg7HiMLw5ubmdjOlPWV1dTXsE4WrjXKQZguZoMAoxG9zc7PJXKLw1H6I5t4iaDYT+jo+Ph72yYQSDlomwLBFkOSwwjb37dsX3gFRyOHa2lr4OlGgZ+aMjWqYYhRoe/To0XCMaA0z+yMKfR2mFnOLAnsz+zBTD0R9ernHfRMFAFBBEQUAUEERBQBQQREFAFBBEQUAUEERBQBQQREFAFBh4DlRUcZRZowoW6KUXIbNMDJ+IplcnVHM3hmk69evh33m5+d7fp1MjsswzM7OdrZnnj16tkz2SuYsj+IZy8wpyqrLZLGNcl7b4uJiT+0Zmecf1ZyoaI9EOVultMkzzLzOsERZfFF7RpRFVkou0y6qKTKv8zS+iQIAqKCIAgCooIgCAKigiAIAqKCIAgCooIgCAKigiAIAqKCIAgCoMLa9vZ3vPDb2h1LKzf5NZ0+a2N7efqkU6/MUj9enFGv0FPZQN+vTzRmL2UPd7KHYn63Rjl0VUQAA/Ik/5wEAVFBEAQBU2NUPEB88eHB7ED8o+stf/rKz/fnnnw/HeO2111pNp9OVK1fu7PydtMX6PHr0KOxz69atzvZPPvkkHGP//v1hn1dffbWz/bnnngvH+OL6lJJbo2gN7t+/H77unTt3OtsfPHgQjvH666+HfTJ7MdJ6D2X89re/7Wz/7LPPwjHeeOONVtPptNv1ifbP+++/3/OcMnt/UD++XHPGItEdXEq8zpkfII7umFJK2bdvX9gn0vqMffDBB2Gfra2tzvbMHZw5Y63Xp5TB3UO3b9/ubP/444/DMd56661W0+n05Brt2FURNTk5WS5fvtxuVk8R/bL3zMxMOMaZM2cazabb2NjY43/4rsX6ZH5NOvpF6nPnzoVjZNYwep3ML7B/cX1Kya1RtAYbGxvh666urna2r6+vh2OcPXs27JNZx0jrPZSxuLjY2X7jxo1wjLW1tUaz6bbb9Yn2T4v3LLP3M+ewhZozFsk8X7TOc3Nz4RjRHVNKrhiLtD5jmWe7ePFiZ/uRI0fCMTJnrPX6lDK4eyj6nM58jg9inqX8+zXa4c95AAAVFFEAABUUUQAAFRRRAAAVFFEAABUUUQAAFRRRAAAVdpUT1UImO2Vzc7OzfVD5NMOQyR+JMlwyGT8LCwthn2vXrvU0j1pRNsjy8nI4xtTUVE+vUcrgwhJbi/JpSonPUOaMZfbZXl3D6A7KPPsoi/ZI9PyllDI9Pd3ZHt0fpeTuu2gvtshJelI098wZO378eGf7yspKOEYmE292djbsMwyZLL6lpaXO9r3wWe+bKACACoooAIAKiigAgAqKKACACoooAIAKiigAgAqKKACACiOZExXla2SyZ/Zqhs29e/fCPtG8M9kamZyTfuVARaKMltOnT4djLC4utprOnjM/Px/2idY4k72T2R/RXuzHHovm3iJXaGtrq+cxhunu3bs9j5G5yyPf+MY3wj5RVlI/cpKifbm6uhqOEWVgZXKiLly4EPYZ1ZyoTBZh9Fnf6tmiufTyeeGbKACACoooAIAKiigAgAqKKACACoooAIAKiigAgAqKKACACoooAIAKzcM2z5w509kehfyVEgf0ZYIiM4GDUSBni1C+3YrWr5Q45C4TNDo+Ph72mZmZCfsMw9LSUs99MiFumfdiGIGt0f7PzCnqkwnBzITpjWKgbRTemDE9Pd1gJsPTIuR0c3Ozs31iYqLn1yillEuXLnW2DyNsMrP3M59TkVG9g0uJ78dMcHTmjo1kgjKjmqKX4FjfRAEAVFBEAQBUUEQBAFRQRAEAVFBEAQBUUEQBAFRQRAEAVBh4TtTc3Fw4RpQltbq6Go6RyUGJ8pRaZKnsViYXpEV2SGaMTN5UP0SZHuvr6z2/xtbWVtgnkwUT7fd+7KGrV6/2PEZ0xqIMoFJyOTBRTlRmjVvLZKRF+yPz7Jk+w8iiKyXel8ePHw/HyNzlLWTWcRRF+VaZfTiM85EV3dMZJ0+e7GzPfAZl5tHPveqbKACACoooAIAKiigAgAqKKACACoooAIAKiigAgAqKKACACoooAIAKuwrb/Pzzz8Pwq5s3b3a2nz9/PnydTJ/I1NRU2GcYQXdRuFhmTouLiz3PIwpbLKWUAwcO9Pw6/dAibLSV6P1sEUj3pCjAMhNQd/jw4c72zPk5d+5c2GcYgbWDEL0HpQwvSLOFKES2lPi9zYRkRuenlNFcx8z9GZ2PzBqPsugezqxRFJy8sbERjjE9PR326eda+yYKAKCCIgoAoIIiCgCggiIKAKCCIgoAoIIiCgCggiIKAKCCIgoAoMKuwjafeeaZMGTu6tWrne2ZALYoGCsT4pXpMwxRUObc3Fw4RvRsCwsL4RhbW1thn4mJibDPMGT2UBRymVmjCxcuZKc0UNHcM88W7cNMYOdeDdLM7P3IxYsXm/SZnZ3teS7DEu2zTBhr5r04evRockaDkwlvjJ4tE9ia2UPRZ26LcOa/JApKzQSpRs+X+RzPvBf9DGz1TRQAQAVFFABABUUUAEAFRRQAQAVFFABABUUUAEAFRRQAQIVd5URltMiOibKSogygURblVayvr4djRGucyVU5ceJEz68zLJmcqCij5tixY+EY4+PjYZ9RzSOLRDlQMzMzA5nHMGTyz27evNnZntkbUX5PKaVMT093tvcz36ZLJmvs/PnzPb9O9PyljGaWVotzP6j8q0we1bBcunSpsz0z92F/TvkmCgCggiIKAKCCIgoAoIIiCgCggiIKAKCCIgoAoIIiCgCggiIKAKDC2Pb2dr7z2NgfSindKXT/8Uxsb2+/VIr1eYrH61OKNXoKe6ib9enmjMXsoW72UOzP1mjHroooAAD+xJ/zAAAq7Oq38w4ePLjd6+/w3L9/P+zzm9/8prP90aNHPc1hx+uvv97ZnvndqitXrtzZ+Yqvxfpknu369eud7Z9++mk4xmuvvRb2afG7XV9cn1LarFHG559/3tl++/btcIyPP/447PPiiy92tmeetfUe+uyzz8I+v//97zvb//jHP4ZjfOUrXwn7fO1rXwv7RFqvTwsffPBB2OfBgwdhn7feequzfd++feEY/Thjt27dCvt89NFHne2ZuUd3cCmlPP/882GfSOs9lPntvOgu379/fzjGK6+8EvYZ1Xv64cOHYZ9f/epXne2ZPZSZZz/WaMeuiqjJycly+fLlniaS+YHd6AeIt7a2eprDjp/+9Ked7ZkfvhwbG3v8d+MW65P5cd3ox0EzBzx69lLa/PDnF9enlDZrlBH9wO6ZM2fCMVZWVsI+b7/9dmd79EPIpbTfQx9++GHY59SpU53t7777bjjG97///bDPT37yk7BPpPX6tBDdUaXk7rroB1gzl38/ztjJkyfDPsvLy53tmSLh7NmzYZ8WP4bdeg9l3pfoc+rIkSPhGEtLS2GfftzTn332Wbly5cqux5mYmHh892Y+hw4fPtzZntlDw/os27GrIgoA+HJ78OBBqfnnpcfGxvowm9Hmn4kCAKigiAIAqODPeQBAp/fvvF/+5n//zeP/+8O7H5a/O/p3ZfG/Lw5xVsOniAIAOr158M1y7X/+6R8Wf/T/HpWv/cPXynf/63eHPKvh8+c8ACDtX6//a3n9K6+XiRcmhj2VoVNEAQBp//Rv/1T+9q/+dtjTGAkD/3NelN9TSpyDlBnj4sWLYZ/V1dXO9hbZEk+KcqAOHDgQjjEx0V39ZzJeTpw4EfaZnp7ubG8RYFYjk/EUrcHiYvx3/NOnT4d91tbWwj6t3b17t7M9kz/zne98p7M9kwGVyfhpkRM1DFG+V+Z+yYjug36dseiOPX/+fDhGdD9m7unMXZXJ22otel8yWYXj4+Od7Zn3NgpWHobPH31e/vn9fy5//z/+vrPfoN6348ePh3368Vm+wzdRAEDKv/yffynf/uq3yyv74zT1/wgUUQBAyj/+2z/6U94XKKIAgNDD//uwvPvhu+Wv/9tfD3sqI0PEAQAQeu6/PFc++V+fDHsaI8U3UQAAFRRRAAAVFFEAABUG/s9ERfkkGdeuXQv7ZHJcjh071vNcdivKRYkyoEqJ8zcmJyfDMTLPHq3zzMxMOEaNKKOlRbbMoUOHwjHm5ubCPsPw4x//uOcx3n333c72TF7ZqVOnep5HP0T7NnMHbW5uNppNt8xZ7YcowymTkRZlrWXOWL/ukF5lPmMi0T7L5N3tZZksvmiNory2UkpZWloK+0SfB73sQ99EAQBU8G/nAQCP7d+/v4yNje36f5f5S8qXjW+iAIDH3nzzzbK9vb3r/2R+6ufLRhEFAFBBEQUAUEERBQBQQREFAFBBEQUAUGEkIw6ioLNWIYgXLlzobJ+dnW3yOl909OjRzvYoFKyUOKAvE1CWkQnL64cohO6FF14Ix4jWILPOmcDFEydOhH1aO3v2bGf7j370o3CM9957r7M9CjwtpZR33nkn7DMM0XufeV+np6c72zc2NsIxRvlf9472f+b9j85pZp0zZzkKTu7HPZ25HyJra2ud7ZlAz8xnXSbUsh+ifxMvs4bRGkRrmLW1tdVknL/EN1EAABUUUQAAFRRRAAAVFFEAABUUUQAAFRRRAAAVFFEAABUGnhMVZYuUUsrS0lLPr5PJDhlGvkaUP5P5FeyxsbG+z6OUXIZLP0TvS4t5ZbK0FhYWwj4zMzM9z6W1b37zm2GfKCfq29/+djjGkSNHwj6/+MUvOtszc92tkydPdrZn3tcoIy1zBod1fjKi7KX5+flwjBbZO5n77tKlS53t/ciJGsR7F+X9lZL7LIzuoH7l/UWf5SsrK3153SdlPssyfWr5JgoAoIIiCgCggiIKAKCCIgoAoIIiCgCggiIKAKCCIgoAoIIiCgCgwsDDNu/duxf2GR8f72zPhJStra1lpzRQUYjb+fPnwzGOHz/e2Z559kzg4Pr6emd7v4ImozVqEZIahQ1mjWLYZsY777zTU3sppfzgBz8I+5w6daqz/ezZs+EYuxXtnxbhgxMTEz2PMUxTU1Od7ZkzFoUtzs3NhWNkQm+HIXr+zP0Z7cPMGBmZz9R+yARnR6L3PxPoGoXrltLf8FTfRAEAVFBEAQBUUEQBAFRQRAEAVFBEAQBUUEQBAFRQRAEAVFBEAQBUGHjY5uHDh8M+UcBWq5CyUZQJ8YvC1aKQzFJK2dzcDPu0CCUcVRcuXAj77NUgzQMHDvTc52c/+1k4xt27d5v02Ysygb8bGxthnyj0dXZ2NjulXYnmnwknjO7pTAjil1n0OZUJVp6fnw/7jOo9lfn8iPZQZv8P+/l9EwUAUEERBQBQQREFAFBBEQUAUEERBQBQQREFAFBBEQUAUGHgOVHXr18P+0RZSXNzc62mM3Iy2RpRLsbS0lI4xokTJ8I+mayYvSqTpXXu3Lm+z6Mf3nvvvbDPqVOnOtu/973vhWNEeWWllPLzn/887LMXZbJpMjlRmftwGK5duxb2mZqa6mzPZGntVZn3/8aNG53ta2tr4Rj9ygkbhMwajY+Pd7YvLi42mk3/+CYKAKCCIgoAoIIiCgCggiIKAKCCIgoAoIIiCgCggiIKAKCCIgoAoMLY9vZ2vvPY2B9KKTf7N509aWJ7e/ulUqzPUzxen1Ks0VPYQ92sTzdnLGYPdbOHYn+2Rjt2VUQBAPAn/pwHAFBBEQUAUEERBQBQQREFAFBBEQUAUEERBQBQQREFAFBBEQUAUEERBQBQ4f8DXIc8DgCB8ooAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 8)) # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05,\n",
    "wspace=0.05)\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(32):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.gray_r)\n",
    "# label the image with the target value\n",
    "if X_test_predicted[i] == y_test[i]:\n",
    "    ax.text(0, 1, X_test_predicted[i], color='green',\n",
    "    bbox=dict(facecolor='white'))\n",
    "else:\n",
    "    ax.text(0, 1, X_test_predicted[i], color='red',\n",
    "    bbox=dict(facecolor='white'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build our a simple neural network using some of the popularly known datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Networks using Keras \n",
    "Before we build a simple Neural Network let's first go over some litterature\n",
    "\n",
    "First let's get some import out of the way\n",
    "\n",
    "```Python\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# Load the data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "```\n",
    "### 1. Define Keras Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network architecture.\n",
    "The first thing to get right is to ensure the input layer has the right number of input features. This can be specified when creating the first layer with the input_dim argument and setting it to 8 for the 8 input variables.\n",
    "\n",
    "**How do we know the number of layers and their types?**\n",
    "\n",
    "This is a very hard question. There are heuristics that we can use and often the best network structure is found through a process of trial and error experimentation. Generally, you need a network large enough to capture the structure of the problem.\n",
    "\n",
    "In this example, we will use a fully-connected network structure with three layers.\n",
    "\n",
    "Fully connected layers are defined using the Dense class. We can specify the number of neurons or nodes in the layer as the first argument, and specify the activation function using the activation argument.\n",
    "\n",
    "We will use the rectified linear unit activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer.\n",
    "\n",
    "It used to be the case that Sigmoid and Tanh activation functions were preferred for all layers. These days, better performance is achieved using the ReLU activation function. We use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n",
    "\n",
    "We can piece it all together by adding each layer:\n",
    "\n",
    " - The model expects rows of data with 8 variables (the input_dim=8 argument)\n",
    " - The first hidden layer has 100 nodes and uses the relu activation function.\n",
    " - The second hidden layer has 50 nodes and uses the relu activation function.\n",
    " - The output layer has 1 node and uses the sigmoid activation function.\n",
    " \n",
    " ```Python\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=8, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "**Note:** The most confusing thing here is that the shape of the input to the model is defined as an argument on the first hidden layer. This means that the line of code that adds the first Dense layer is doing 2 things, defining the input or visible layer and the first hidden layer\n",
    "\n",
    "## 2. Compile Keras Model\n",
    "Now that the model is defined, we can compile it.\n",
    "\n",
    "Compiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow. The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU or GPU or even distributed.\n",
    "\n",
    "When compiling, we must specify some additional properties required when training the network. Remember training a network means finding the best set of weights to map inputs to outputs in our dataset.\n",
    "\n",
    "We must specify the `loss function` to use to evaluate a set of weights, the `optimizer` is used to search through different weights for the network and any optional metrics we would like to collect and report during training.\n",
    "\n",
    "\n",
    "As part of the optimization algorithm, the error for the current state of the model must be estimated repeatedly. This requires the choice of an error function, conventionally called a loss function, that can be used to estimate the loss of the model so that the weights can be updated to reduce the loss on the next evaluation.\n",
    "\n",
    "Neural network models learn a mapping from inputs to outputs from examples and the choice of loss function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen loss function.\n",
    "\n",
    "**Some choices of the loss functions**\n",
    "1. Regression Loss Functions\n",
    "   - Mean Squared Error Loss\n",
    "   - Mean Squared Logarithmic Error Loss\n",
    "   - ean Absolute Error Loss\n",
    " \n",
    "2. Binary Classification Loss Functions\n",
    "   - Binary Cross-Entropy\n",
    "   - Hinge Loss\n",
    "   - Squared Hinge Loss\n",
    "   \n",
    "3. Multi-Class Classification Loss Functions\n",
    "   - Multi-Class Cross-Entropy Loss\n",
    "   - Sparse Multiclass Cross-Entropy Loss\n",
    "   - Kullback Leibler Divergence Loss\n",
    "\n",
    "for more information on loss functions you can read more [here](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "and the `keras` documentation [keras](https://keras.io/api/losses/)\n",
    "\n",
    "In this case, we will use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as `binary_crossentropy`.\n",
    "\n",
    "We will define the optimizer as the efficient stochastic gradient descent algorithm `adam`. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems. To learn more about the Adam version of stochastic gradient descent see the this [post](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "Finally, because it is a classification problem, we will collect and report the classification accuracy, defined via the metrics argument.\n",
    "\n",
    "```Python\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "## 3. Train Keras Model\n",
    "We have defined our model and compiled it ready for efficient computation.\n",
    "\n",
    "Now it is time to execute the model on some data.\n",
    "\n",
    "We can train or fit our model on our loaded data by calling the fit() function on the model.\n",
    "\n",
    "Training occurs over epochs and each epoch is split into batches.\n",
    "\n",
    "`Epoch`: One pass through all of the rows in the training dataset.\n",
    "\n",
    "`Batch`: One or more samples considered by the model within an epoch before weights are updated.\n",
    "One epoch is comprised of one or more batches, based on the chosen batch size and the model is fit for many epochs. For more on the difference between epochs and batches read [here](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)\n",
    "\n",
    "The training process will run for a fixed number of iterations through the dataset called epochs, that we must specify using the epochs argument. We must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size and set using the batch_size argument.\n",
    "\n",
    "For this problem, we will run for a small number of epochs (150) and use a relatively small batch size of 10.\n",
    "\n",
    "These configurations can be chosen experimentally by trial and error. We want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration. This is called model convergence.\n",
    "\n",
    "```Python\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10)\n",
    "```\n",
    "This is where the work happens on your CPU or GPU, however here no GPUs needed for this example\n",
    "\n",
    "\n",
    "## 4. Evaluate Keras Model\n",
    "We have trained our neural network on the entire dataset and we can evaluate the performance of the network on the same dataset.\n",
    "\n",
    "This will only give us an idea of how well we have modeled the dataset (e.g. train accuracy), but no idea of how well the algorithm might perform on new data. We have done this for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "You can evaluate your model on your training dataset using the evaluate() function on your model and pass it the same input and output used to train the model.\n",
    "\n",
    "This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n",
    "\n",
    "The evaluate() function will return a list with two values. The first will be the loss of the model on the dataset and the second will be the accuracy of the model on the dataset. We are only interested in reporting the accuracy, so we will ignore the loss value.\n",
    "\n",
    "```python\n",
    "# Evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "```\n",
    "\n",
    "## 5. Tie It All Together\n",
    "We have just seen how we can easily create our first neural network model in Keras.\n",
    "\n",
    "Let’s tie it all together into a complete code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4879 - accuracy: 0.7630\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4809 - accuracy: 0.7721\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4656 - accuracy: 0.7891\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5074 - accuracy: 0.7591\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7917\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4867 - accuracy: 0.7604\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4861 - accuracy: 0.7695\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.7617\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4759 - accuracy: 0.7747\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.7669\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4831 - accuracy: 0.7721\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5071 - accuracy: 0.7539\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4742 - accuracy: 0.7799\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7891\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.7643\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4643 - accuracy: 0.7734\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4664 - accuracy: 0.7721\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.7734\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.7773\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4705 - accuracy: 0.7669\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4673 - accuracy: 0.7826\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4654 - accuracy: 0.7747\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.7930\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4767 - accuracy: 0.7747\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4710 - accuracy: 0.7682\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.7643\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7734\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4741 - accuracy: 0.7826\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4710 - accuracy: 0.7812\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.7852\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7839\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4618 - accuracy: 0.7695\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.7786\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.7760\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.7604\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.7826\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.7682\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.7904\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.7956\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4899 - accuracy: 0.7682\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.7852\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.7826\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.7721\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.7721\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.7656\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7878\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4634 - accuracy: 0.7695\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7839\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4650 - accuracy: 0.7760\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.7669\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4526 - accuracy: 0.7904\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7839\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.8008\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.7865\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.7812\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.7852\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4496 - accuracy: 0.7826\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4448 - accuracy: 0.7878\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7773\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4512 - accuracy: 0.7786\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7656\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.7747\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.7643\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4997 - accuracy: 0.7578\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4516 - accuracy: 0.7878\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4452 - accuracy: 0.7852\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4757 - accuracy: 0.7904\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.7708\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.7812\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4546 - accuracy: 0.7826\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7786\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.7943\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7904\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.7786\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4433 - accuracy: 0.7839\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.7943\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4627 - accuracy: 0.7812\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.7799\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.7721\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.7917\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.7812\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.7917\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4349 - accuracy: 0.7904\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.7995\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.7786\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.7799\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.7904\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.7786\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.7956\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4503 - accuracy: 0.7826\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4516 - accuracy: 0.7773\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4639 - accuracy: 0.7839\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.7839\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.7839\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 0s 1000us/step - loss: 0.4439 - accuracy: 0.7852\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4509 - accuracy: 0.7812\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.7956\n",
      "Epoch 98/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.7786\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4470 - accuracy: 0.7826\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.7878\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.7865\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.7852\n",
      "Epoch 103/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.7930\n",
      "Epoch 104/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4420 - accuracy: 0.7982\n",
      "Epoch 105/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4468 - accuracy: 0.7812\n",
      "Epoch 106/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.7995\n",
      "Epoch 107/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4454 - accuracy: 0.7878\n",
      "Epoch 108/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7682\n",
      "Epoch 109/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.7904\n",
      "Epoch 110/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.7826\n",
      "Epoch 111/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4490 - accuracy: 0.7747\n",
      "Epoch 112/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.7943\n",
      "Epoch 113/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4465 - accuracy: 0.7865\n",
      "Epoch 114/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4412 - accuracy: 0.7904\n",
      "Epoch 115/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4395 - accuracy: 0.7799: 0s - loss: 0.4448 - accuracy: 0.\n",
      "Epoch 116/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.7799\n",
      "Epoch 117/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.7930\n",
      "Epoch 118/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.7865\n",
      "Epoch 119/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.7891\n",
      "Epoch 120/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.7786\n",
      "Epoch 121/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.7917\n",
      "Epoch 122/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.7812\n",
      "Epoch 123/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.7891\n",
      "Epoch 124/150\n",
      "77/77 [==============================] - 0s 986us/step - loss: 0.4469 - accuracy: 0.7839\n",
      "Epoch 125/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.7878\n",
      "Epoch 126/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4613 - accuracy: 0.7812\n",
      "Epoch 127/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4332 - accuracy: 0.7982\n",
      "Epoch 128/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.7969\n",
      "Epoch 129/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.7956\n",
      "Epoch 130/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.7839\n",
      "Epoch 131/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4346 - accuracy: 0.7956\n",
      "Epoch 132/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.7956\n",
      "Epoch 133/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.7956\n",
      "Epoch 134/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.7839\n",
      "Epoch 135/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4290 - accuracy: 0.7878\n",
      "Epoch 136/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.7878\n",
      "Epoch 137/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7812\n",
      "Epoch 138/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4424 - accuracy: 0.7930\n",
      "Epoch 139/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.7930\n",
      "Epoch 140/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.7956\n",
      "Epoch 141/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.7891\n",
      "Epoch 142/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.7995\n",
      "Epoch 143/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.7839\n",
      "Epoch 144/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.7904\n",
      "Epoch 145/150\n",
      "77/77 [==============================] - 0s 960us/step - loss: 0.4327 - accuracy: 0.7930\n",
      "Epoch 146/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.7917\n",
      "Epoch 147/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.7695\n",
      "Epoch 148/150\n",
      "77/77 [==============================] - 0s 896us/step - loss: 0.4296 - accuracy: 0.7930\n",
      "Epoch 149/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.7943\n",
      "Epoch 150/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.7891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x200b4107788>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4085 - accuracy: 0.8008\n",
      "Accuracy: 80.08\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictions \n",
    "\n",
    "We can adapt the above example and use it to generate predictions on the training dataset, pretending it is a new dataset we have not seen before.\n",
    "\n",
    "Making predictions is as easy as calling the predict() function on the model. We are using a sigmoid activation function on the output layer, so the predictions will be a probability in the range between 0 and 1. We can easily convert them into a crisp binary prediction for this classification task by rounding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n",
    "# round predictions \n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "\n",
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n",
    "# round predictions \n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, we can call the predict_classes() function on the model to predict crisp classes directly, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 \n",
    "Mushroom classification using neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 \n",
    "Iris dataset classification using neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names) \n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = data.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=4, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1000)              5000      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 3003      \n",
      "=================================================================\n",
      "Total params: 1,009,003\n",
      "Trainable params: 1,009,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.9567 - accuracy: 0.5250 - val_loss: 0.6502 - val_accuracy: 0.7667\n",
      "Epoch 2/8\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5939 - accuracy: 0.7500 - val_loss: 0.5875 - val_accuracy: 0.6000\n",
      "Epoch 3/8\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4647 - accuracy: 0.7583 - val_loss: 0.3148 - val_accuracy: 0.9667\n",
      "Epoch 4/8\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4357 - accuracy: 0.7667 - val_loss: 0.2671 - val_accuracy: 0.9000\n",
      "Epoch 5/8\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3460 - accuracy: 0.8167 - val_loss: 0.2713 - val_accuracy: 0.9000\n",
      "Epoch 6/8\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2539 - accuracy: 0.9000 - val_loss: 0.2078 - val_accuracy: 0.9667\n",
      "Epoch 7/8\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2070 - accuracy: 0.9667 - val_loss: 0.1903 - val_accuracy: 0.9667\n",
      "Epoch 8/8\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.1782 - accuracy: 0.9667 - val_loss: 0.1844 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x200b55386c8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us, deep-dive, into the top 10 deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of neural networks\n",
    "\n",
    "There are different kinds of deep neural networks – and each has advantages and disadvantages, depending upon the use. Examples include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.sas.com/en_za/insights/analytics/neural-networks.html#:~:text=What%20they%20are%20and%20why,time%20%E2%80%93%20continuously%20learn%20and%20improve.\n",
    "\n",
    "2. https://dzone.com/articles/comparison-between-deep-learning-vs-machine-learni\n",
    "3. https://www.zendesk.com/blog/machine-learning-and-deep-learning/#:~:text=To%20recap%20the%20differences%20between,intelligent%20decisions%20on%20its%20own\n",
    "4. https://machinelearningmastery.com/what-is-deep-learning/\n",
    "5. https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-algorithm#:~:text=Deep%20learning%20uses%20artificial%20neural,machines%20by%20learning%20from%20examples.\n",
    "\n",
    "6. https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf\n",
    "7. https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "8. https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
